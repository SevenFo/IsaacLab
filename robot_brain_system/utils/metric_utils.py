from typing import List
import functools
import time
import torch


def get_total_gpu_memory_allocated_mb() -> float:
    """è®¡ç®—å¹¶è¿”å›æ‰€æœ‰å¯ç”¨CUDAè®¾å¤‡ä¸Šå·²åˆ†é…æ˜¾å­˜çš„æ€»å’Œï¼ˆå•ä½MBï¼‰ã€‚"""
    if not torch.cuda.is_available():
        return 0.0
    total_mem = 0
    for i in range(torch.cuda.device_count()):
        total_mem += torch.cuda.memory_allocated(device=i)
    return total_mem / 1024**2


# --- æ–°å¢ï¼šæ€§èƒ½æŒ‡æ ‡è£…é¥°å™¨ï¼ˆå·²å‡çº§ä¸ºå¤šGPUæ„ŸçŸ¥ï¼‰ ---
class with_metrics:
    """
    ä¸€ä¸ªç”¨äºæµ‹é‡å‡½æ•°æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æ‰§è¡Œæ—¶é—´ã€GPUæ˜¾å­˜æ¶ˆè€—ï¼‰çš„è£…é¥°å™¨ã€‚
    """

    def __init__(self, metrics: List[str]):
        valid_metrics = ["time", "gpu_memory"]
        if not all(m in valid_metrics for m in metrics):
            raise ValueError(f"æŒ‡å®šçš„æŒ‡æ ‡æ— æ•ˆã€‚è¯·ä» {valid_metrics} ä¸­é€‰æ‹©ã€‚")
        self.metrics = metrics

    def __call__(self, func):
        decorator_self = self

        @functools.wraps(func)
        def wrapper(self, *args, **kwargs):
            start_time = None
            initial_gpu_mems = []
            use_time = "time" in decorator_self.metrics
            use_gpu = (
                "gpu_memory" in decorator_self.metrics and torch.cuda.is_available()
            )

            # --- å‡†å¤‡æŒ‡æ ‡æ”¶é›† ---
            if use_time:
                start_time = time.perf_counter()

            if use_gpu:
                # éå†æ‰€æœ‰è®¾å¤‡ï¼Œé‡ç½®å³°å€¼ç»Ÿè®¡å¹¶è®°å½•åˆå§‹æ˜¾å­˜
                for i in range(torch.cuda.device_count()):
                    torch.cuda.reset_peak_memory_stats(device=i)
                    initial_gpu_mems.append(torch.cuda.memory_allocated(device=i))

            # --- æ‰§è¡ŒåŸå§‹å‡½æ•° ---
            result = func(self, *args, **kwargs)

            # --- æ”¶é›†å¹¶æ‰“å°æŒ‡æ ‡ ---
            print(f"\n--- ğŸ“Š Metrics for '{func.__name__}' ---")

            if use_time and start_time is not None:
                end_time = time.perf_counter()
                elapsed = end_time - start_time
                print(f"â±ï¸  Execution Time: {elapsed:.4f} seconds")

            if use_gpu:
                peak_gpu_mems = []
                # éå†æ‰€æœ‰è®¾å¤‡ï¼Œè·å–å³°å€¼æ˜¾å­˜
                for i in range(torch.cuda.device_count()):
                    peak_gpu_mems.append(torch.cuda.max_memory_allocated(device=i))

                total_initial_mem = sum(initial_gpu_mems)
                total_peak_mem = sum(peak_gpu_mems)
                gpu_mem_consumed = total_peak_mem - total_initial_mem

                print(
                    f"ğŸ’¾  Total GPU Memory Consumed (Delta): {gpu_mem_consumed / 1024**2:.2f} MB"
                )
                print(
                    f"    (Initial Total: {total_initial_mem / 1024**2:.2f} MB -> Peak Total: {total_peak_mem / 1024**2:.2f} MB)"
                )

                if torch.cuda.device_count() > 1:
                    print("    Per-GPU Breakdown (Initial -> Peak) MB:")
                    for i in range(torch.cuda.device_count()):
                        initial_mb = initial_gpu_mems[i] / 1024**2
                        peak_mb = peak_gpu_mems[i] / 1024**2
                        delta_mb = peak_mb - initial_mb
                        print(
                            f"      GPU {i}: {initial_mb:.2f} -> {peak_mb:.2f} (Delta: {delta_mb:+.2f})"
                        )

            print("---------------------------------")

            return result

        return wrapper
