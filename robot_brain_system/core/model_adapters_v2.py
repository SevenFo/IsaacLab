# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€çš„æ¨¡å‹é€‚é…å™¨æ¨¡å—ï¼ˆå·²ä¿®æ­£ç‰ˆæœ¬ï¼‰ï¼Œä¸ºä¸åŒæ¨ç†åç«¯æä¾›æ ‡å‡†åŒ–æ¥å£ã€‚
"""

import base64
from abc import ABC, abstractmethod
import copy
from io import BytesIO
import json
from typing import Any, Dict, List, Optional, Tuple
import argparse

import torch
import os
from PIL import Image

from robot_brain_system.utils.metric_utils import (
    with_metrics,
    get_total_gpu_memory_allocated_mb,
)
from robot_brain_system.utils.retry_utils import retry

# --- ä¾èµ–å¯¼å…¥ï¼Œå¸¦é”™è¯¯å¤„ç† ---
try:
    from transformers import (
        AutoConfig,
        AutoModelForCausalLM,
        AutoProcessor,
    )

    # æ˜¾å¼å¯¼å…¥ Qwen2.5-VL çš„æ¨¡å‹ç±»ï¼Œç¡®ä¿é€»è¾‘å®Œæ•´
    from transformers.models.qwen2_5_vl import Qwen2_5_VLForConditionalGeneration
    from transformers.models.glm4v import Glm4vForConditionalGeneration
    from qwen_vl_utils import process_vision_info

    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    import openai
    from openai import APIError, RateLimitError, AuthenticationError

    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from lmdeploy import (
        pipeline,
        TurbomindEngineConfig,
        ChatTemplateConfig,
        GenerationConfig,
    )
    from lmdeploy.vl.constants import IMAGE_TOKEN

    LMDEPLOY_AVAILABLE = True
except ImportError:
    LMDEPLOY_AVAILABLE = False
    IMAGE_TOKEN = "<image>"

try:
    from vllm import LLM, SamplingParams

    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False


# --- æŠ½è±¡åŸºç±» ---
class BaseModelAdapter(ABC):
    @abstractmethod
    def generate(
        self,
        history: List[Dict[str, Any]],
        max_tokens: int = 2048,
        **kwargs,
    ) -> Tuple[str, Any]:
        pass


# --- å››ç§é€‚é…å™¨å®ç°ï¼ˆä¿®æ­£ç‰ˆï¼‰ ---


class TransformersAdapter(BaseModelAdapter):
    """
    åŸºäº Hugging Face `transformers` åº“çš„é€‚é…å™¨ã€‚
    """

    def __init__(self, model_path: str, device: str = "auto"):
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "`transformers` å’Œ `qwen_vl_utils` æ˜¯ä½¿ç”¨ TransformersAdapter çš„å¿…è¦ä¾èµ–ã€‚"
            )

        self.model_path = model_path
        self.device = device

        # FIX: ä¿®æ­£äº†æ¨¡å‹åŠ è½½é€»è¾‘ï¼Œä½¿ç”¨ config åˆ¤æ–­å¹¶åŠ è½½æ­£ç¡®çš„æ¨¡å‹ç±»
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "").lower()
        print(model_type)
        _initial_total_memory_mb = get_total_gpu_memory_allocated_mb()
        if "qwen2_5_vl" in model_type:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype="float16",
                device_map=device,
                trust_remote_code=True,
            )
            # limit pic token from 256 to 1280
            self.processor = AutoProcessor.from_pretrained(
                model_path,
                min_pixels=64 * 28 * 28,  # 224^2
                max_pixels=1280 * 28 * 28,  # 1000^2
            )
            self._handler = self._generate_qwen
            print("[TransformersAdapter] å·²åŠ è½½ Qwen2.5-VL æ¨¡å‹ã€‚")
        elif "glm4v" in model_type:
            self.model = Glm4vForConditionalGeneration.from_pretrained(
                model_path, torch_dtype=torch.bfloat16, device_map=device
            )
            self.processor = AutoProcessor.from_pretrained(model_path, use_fast=True)
            self._handler = self._generate_generic

            print("[TransformersAdapter] å·²åŠ è½½ GLM-4V æ¨¡å‹ã€‚")
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map=device,
                trust_remote_code=True,
            )
            self.processor = AutoProcessor.from_pretrained(model_path, use_fast=True)
            self._handler = self._generate_generic
            print(f"[TransformersAdapter] å·²åŠ è½½é€šç”¨æ¨¡å‹: {model_type}ã€‚")
        _loaded_total_memory_mb = get_total_gpu_memory_allocated_mb()
        _cost_mb = _loaded_total_memory_mb - _initial_total_memory_mb
        print(
            f"[TransformersAdapter] æ¨¡å‹å·²é€šè¿‡ `device_map='{device}'` åˆ†å¸ƒåˆ°å¯ç”¨è®¾å¤‡ã€‚"
        )
        print(f"[TransformersAdapter] æ¨¡å‹åŠ è½½å ç”¨æ€»æ˜¾å­˜: {_cost_mb:.2f} MB")

    def _generate_qwen(
        self, history: List[Dict[str, Any]], gen_kwargs: Dict
    ) -> Tuple[torch.Tensor, Dict]:
        text_prompt = self.processor.apply_chat_template(
            history, tokenize=False, add_generation_prompt=True
        )
        vision_outputs = process_vision_info(history)
        images, videos = vision_outputs[0], vision_outputs[1]

        inputs = self.processor(
            text=[text_prompt],
            images=images,
            videos=videos,
            padding=True,
            return_tensors="pt",
        ).to(self.model.device)

        generated_ids = self.model.generate(**inputs, **gen_kwargs)
        return generated_ids, inputs

    def _generate_generic(
        self, history: List[Dict[str, Any]], gen_kwargs: Dict
    ) -> Tuple[torch.Tensor, Dict]:
        inputs = self.processor.apply_chat_template(
            history,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
            padding=True,
            do_sample_frames=True,
        ).to(self.model.device)
        generated_ids = self.model.generate(**inputs, **gen_kwargs)
        return generated_ids, inputs

    @with_metrics(metrics=["time", "gpu_memory"])
    def generate(
        self, history: List[Dict[str, Any]], max_tokens: int = 2048, **kwargs
    ) -> Tuple[str, Any]:
        print(f"\n\n{'=' * 20}input{'=' * 20}")
        print(f"{history}")
        _initial_memory = torch.cuda.memory_allocated()
        gen_kwargs = {
            "max_new_tokens": max_tokens,
            # "top_k": 2,
            # "repetition_penalty": 1.0,
            # "temperature": 0.1,
            # "do_sample": True,
        }
        gen_kwargs.update(kwargs)

        generated_ids, inputs = self._handler(history, gen_kwargs)

        input_token_len = inputs.input_ids.shape[1]
        response_ids = generated_ids[0, input_token_len:]
        response = self.processor.decode(response_ids, skip_special_tokens=True)

        print(f"\n\n{'=' * 20}response{'=' * 20}")
        print(f"{response.strip()}")

        return response.strip(), generated_ids


# --- MODIFIED VLLMAdapter ---
class VLLMAdapter(BaseModelAdapter):
    """
    åŸºäº `vLLM` çš„é«˜æ•ˆæ¨ç†é€‚é…å™¨ã€‚
    è¯¥ç‰ˆæœ¬å¯¹ Qwen ç³»åˆ—æ¨¡å‹è¿›è¡Œäº†ç‰¹æ®Šå¤„ç†ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚
    """

    def __init__(self, model_path: str, **vllm_kwargs):
        if not VLLM_AVAILABLE:
            raise ImportError("`vllm` æ˜¯ä½¿ç”¨ VLLMAdapter çš„å¿…è¦ä¾èµ–ã€‚")
        vllm_kwargs.setdefault("trust_remote_code", True)
        self.llm = LLM(model=model_path, **vllm_kwargs)
        self.model_path = model_path

        # --- NEW: Qwen-specific setup ---
        self.is_qwen_model = "qwen" in model_path.lower()
        if self.is_qwen_model:
            print(
                "[VLLMAdapter] Qwen model detected. Initializing Qwen-specific processor."
            )
            self.processor = AutoProcessor.from_pretrained(model_path)
            # For Qwen, the processor's tokenizer is the source of truth
            self.tokenizer = self.processor.tokenizer
        else:
            print("[VLLMAdapter] Initializing with generic vLLM tokenizer.")
            self.processor = None
            self.tokenizer = self.llm.get_tokenizer()

    def _prepare_vllm_input(self, history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å°†å†…éƒ¨å†å²è®°å½•æ ¼å¼è½¬æ¢ä¸º vLLM æ‰€éœ€çš„è¾“å…¥æ ¼å¼ã€‚
        å¯¹ Qwen æ¨¡å‹ä½¿ç”¨ä¸“é—¨çš„å¤„ç†æµç¨‹ã€‚
        """
        # --- NEW: Qwen-specific input preparation ---
        if self.is_qwen_model:
            # For Qwen, we use its processor to create the prompt from the rich history format.
            # The `process_vision_info` utility then extracts the image/video data.
            # NOTE: Our internal format stores video as a list of frames (PIL.Image).
            # The `qwen_vl_utils.process_vision_info` expects a video path/URL.
            # To bridge this, we treat video frames as a sequence of images, which is a
            # robust and compatible approach for multi-image models like Qwen-VL.

            # Step 1: Transform our 'video' entries into multiple 'image' entries.
            transformed_history = []
            for message in history:
                new_content = []
                for item in message.get("content", []):
                    if item.get("type") == "video" and isinstance(
                        item.get("video"), list
                    ):
                        # Add a text hint that these are video frames
                        new_content.append(
                            {
                                "type": "text",
                                "text": "The following images are sequential frames from a video:",
                            }
                        )
                        for frame in item["video"]:
                            if isinstance(frame, Image.Image):
                                new_content.append({"type": "image", "image": frame})
                    else:
                        new_content.append(item)
                transformed_history.append(
                    {"role": message["role"], "content": new_content}
                )

            # Step 2: Use Qwen's tools to process the transformed history
            prompt = self.processor.apply_chat_template(
                transformed_history,
                tokenize=False,
                add_generation_prompt=True,
            )
            image_inputs, video_inputs, video_kwargs = process_vision_info(
                transformed_history, return_video_kwargs=True
            )

            mm_data = {}
            if image_inputs:
                mm_data["image"] = image_inputs
            # video_inputs will be None due to our transformation, which is expected.
            if video_inputs:
                mm_data["video"] = video_inputs

            return {
                "prompt": prompt,
                "multi_modal_data": mm_data if mm_data else None,
                "mm_processor_kwargs": video_kwargs,
            }

        # --- Fallback to generic processing for other models ---
        else:
            images, processed_history = [], []
            for message in history:
                role, content = message["role"], message.get("content", [])
                new_content_parts = []
                for item in content:
                    if item["type"] == "text":
                        new_content_parts.append(item["text"])
                    elif item["type"] in ["image", "video"]:
                        # Flatten all frames from images or videos into a single list
                        frames = (
                            item.get("video")
                            if item.get("type") == "video"
                            else [item.get("image")]
                        )
                        if frames:
                            for frame in frames:
                                if frame:
                                    new_content_parts.append("<image>")
                                    images.append(frame)
                processed_history.append(
                    {"role": role, "content": "".join(new_content_parts)}
                )

            prompt = self.tokenizer.apply_chat_template(
                conversation=processed_history,
                tokenize=False,
                add_generation_prompt=True,
            )
            return {
                "prompt": prompt,
                "multi_modal_data": {"image": images} if images else None,
            }

    def generate(
        self, history: List[Dict[str, Any]], max_tokens: int = 2048, **kwargs
    ) -> Tuple[str, Any]:
        vllm_input = self._prepare_vllm_input(history)
        sampling_params = SamplingParams(max_tokens=max_tokens, **kwargs)

        # Unpack the prepared inputs for the generate call
        outputs = self.llm.generate(
            prompts=[vllm_input["prompt"]],
            sampling_params=sampling_params,
            multi_modal_data=vllm_input.get("multi_modal_data"),
            # Pass special kwargs if they exist (for Qwen video)
            mm_processor_kwargs=vllm_input.get("mm_processor_kwargs"),
        )
        response = outputs[0].outputs[0].text
        return response.strip(), outputs


class LMDeployAdapter(BaseModelAdapter):
    """åŸºäº `lmdeploy` pipeline çš„æ¨ç†é€‚é…å™¨ï¼ˆå·²ä¿®æ­£æ•°æ®è½¬æ¢é€»è¾‘ï¼‰ã€‚"""

    def __init__(self, model_path: str, **pipeline_kwargs):
        if not LMDEPLOY_AVAILABLE:
            raise ImportError("`lmdeploy` æ˜¯ä½¿ç”¨ LMDeployAdapter çš„å¿…è¦ä¾èµ–ã€‚")
        self.pipe = pipeline(
            model_path,
            backend_config=TurbomindEngineConfig(
                session_len=4096 * 2, device_num=2, dp=1, tp=2, **pipeline_kwargs
            ),
            chat_template_config=ChatTemplateConfig(model_name="internvl2_5"),
        )

    def convert_video_to_images(self, messages):
        """
        å°†æ¶ˆæ¯ä¸­çš„videoç±»å‹è½¬æ¢ä¸ºimageç±»å‹
        """
        converted_messages = []

        for message in messages:
            converted_message = message.copy()

            if "content" in message:
                converted_content = []

                for content_item in message["content"]:
                    if content_item.get("type") == "video":
                        # æå–è§†é¢‘å¸§
                        video_data = content_item["video"]
                        frames = (
                            video_data if isinstance(video_data, list) else [video_data]
                        )

                        # æ·»åŠ æ–‡æœ¬è¯´æ˜
                        converted_content.append(
                            {
                                "type": "text",
                                "text": "The following is the observation of the video frame sequence of the current scene:",
                            }
                        )

                        # å°†æ¯ä¸€å¸§è½¬æ¢ä¸ºå›¾åƒ
                        for i, frame in enumerate(frames):
                            # å¯é€‰ï¼šæ·»åŠ å¸§åºå·è¯´æ˜
                            converted_content.append(
                                {"type": "text", "text": f"Frame{i + 1}/{len(frames)}"}
                            )
                            converted_content.append(
                                {
                                    "type": "image_data",
                                    "image_data": {
                                        "data": frame,
                                        "max_dynamic_patch": 12,
                                    },
                                }
                            )
                    elif content_item.get("type") == "image":
                        # ä¿æŒå›¾åƒç±»å‹ä½†æ˜¯ä¿®æ”¹ä¸ºimage_data
                        frame = content_item["image"]
                        converted_content.append(
                            {
                                "type": "image_data",
                                "image_data": {"data": frame, "max_dynamic_patch": 12},
                            }
                        )
                    else:
                        # ä¿æŒå…¶ä»–ç±»å‹ä¸å˜
                        converted_content.append(content_item)

                converted_message["content"] = converted_content

            converted_messages.append(converted_message)

        return converted_messages

    def generate(
        self, history: List[Dict[str, Any]], max_tokens: int = 2048, **kwargs
    ) -> Tuple[str, Any]:
        # FIX: lmdeploy çš„ pipeline å¯ä»¥ç›´æ¥å¤„ç†æ ‡å‡† history æ ¼å¼ï¼Œæ— éœ€å¤æ‚è½¬æ¢
        # å®ƒä¼šè‡ªåŠ¨å¤„ç† content åˆ—è¡¨ä¸­çš„æ–‡æœ¬å’Œå›¾åƒ
        history = self.convert_video_to_images(history)
        print("-------- Input messages --------")
        print(history)
        gen_args = {
            "top_k": 0,
            "top_p": 0.8,
            "temperature": 0.8,
            "max_new_tokens": max_tokens,
            "do_sample": True,
        }
        gen_args.update(kwargs)
        gen_config = GenerationConfig(**gen_args)
        output = self.pipe(history, gen_config=gen_config)
        if hasattr(output, "text"):
            response = output.text  # type: ignore
        elif isinstance(output, list) and len(output) > 0:
            first_item = output[0]
            response = (
                first_item.text if hasattr(first_item, "text") else str(first_item)
            )  # type: ignore
        else:
            response = str(output)
        print(f"-------- Generated response --------\n{response.strip()}\n")
        return response.strip(), output


class OpenAIAdapter(BaseModelAdapter):
    """
    é€‚é…å™¨ï¼Œç”¨äºè°ƒç”¨ç¬¦åˆ OpenAI API è§„èŒƒçš„æ¨¡å‹æœåŠ¡ï¼ˆå¦‚ vLLM serverï¼‰ã€‚
    è¯¥ç‰ˆæœ¬æ”¯æŒå¯¹è§†é¢‘è¿›è¡Œä¸¤ç§ä¸åŒçš„å¤„ç†ç­–ç•¥ï¼Œå¯é€šè¿‡å‚æ•°åˆ‡æ¢ã€‚
    """

    def __init__(
        self,
        model_name: str,
        api_key: str = "EMPTY",
        base_url: Optional[str] = None,
        video_conversion_strategy: str = "as_images",  # ## NEW: æ·»åŠ ç­–ç•¥åˆ‡æ¢å‚æ•°
    ):
        """
        åˆå§‹åŒ– OpenAIAdapterã€‚

        Args:
            model_name (str): è¦è°ƒç”¨çš„æ¨¡å‹åç§°ã€‚
            api_key (str, optional): API å¯†é’¥. é»˜è®¤ä¸º "EMPTY".
            base_url (Optional[str], optional): API çš„åŸºç¡€URL. é»˜è®¤ä¸º None.
            video_conversion_strategy (str, optional): è§†é¢‘å¤„ç†ç­–ç•¥ã€‚
                - "as_images": å°†è§†é¢‘å¸§ä½œä¸ºä¸€ç³»åˆ—å›¾åƒå‘é€ (é»˜è®¤, å…¼å®¹æ€§å¥½)ã€‚
                - "as_video_url": å°†æ•´ä¸ªè§†é¢‘æ–‡ä»¶ä½œä¸º Base64 URL å‘é€ (éœ€è¦vLLMç­‰åç«¯æ”¯æŒ)ã€‚
                é»˜è®¤ä¸º "as_images".
        """
        if not OPENAI_AVAILABLE:
            raise ImportError("`openai` æ˜¯ä½¿ç”¨ OpenAIAdapter çš„å¿…è¦ä¾èµ–ã€‚")
        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)
        self.model_name = self.client.models.list().data[0].id
        print(f"[OpenAIAdapter] å·²è¿æ¥åˆ° OpenAI API: {self.model_name}")
        # ## NEW: éªŒè¯å¹¶è®¾ç½®ç­–ç•¥
        if video_conversion_strategy not in ["as_images", "as_video_url"]:
            raise ValueError(
                "video_conversion_strategy å¿…é¡»æ˜¯ 'as_images' æˆ– 'as_video_url'"
            )
        self.video_conversion_strategy = video_conversion_strategy
        print(f"[OpenAIAdapter] è§†é¢‘å¤„ç†ç­–ç•¥å·²è®¾ç½®ä¸º: {self.video_conversion_strategy}")

    def _data_to_base64_url(self, data_bytes: bytes, mime_type: str) -> str:
        """é€šç”¨å‡½æ•°ï¼Œå°†åŸå§‹å­—èŠ‚æ•°æ®ç¼–ç ä¸º Base64 data URLã€‚"""
        encoded_str = base64.b64encode(data_bytes).decode("utf-8")
        return f"data:{mime_type};base64,{encoded_str}"

    def _image_to_base64_url(self, image: Image.Image) -> str:
        """å°† PIL Image å¯¹è±¡è½¬æ¢ä¸º JPEG Base64 data URLã€‚"""
        buffered = BytesIO()
        image.convert("RGB").save(buffered, format="JPEG")
        return self._data_to_base64_url(buffered.getvalue(), "image/jpeg")

    def _convert_history_to_openai_input(
        self, history: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        å°†å†…éƒ¨å†å²è®°å½•æ ¼å¼è½¬æ¢ä¸º OpenAI API æ‰€éœ€çš„ `messages` æ ¼å¼ã€‚
        ## MODIFIED: æ­¤å‡½æ•°ç°åœ¨ä¼šæ ¹æ® video_conversion_strategy é€‰æ‹©ä¸åŒçš„è§†é¢‘å¤„ç†æ–¹å¼ã€‚
        """
        openai_messages = []
        for message in history:
            role = message["role"]
            content = message.get("content", [])

            if role in ["system", "assistant"]:
                text_content = "".join(
                    [item["text"] for item in content if item["type"] == "text"]
                )
                openai_messages.append({"role": role, "content": text_content})
                continue

            if role == "user":
                openai_content_parts = []
                for item in content:
                    item_type = item.get("type")

                    if item_type == "text":
                        openai_content_parts.append(
                            {"type": "text", "text": item["text"]}
                        )

                    elif item_type == "image" and item.get("image"):
                        base64_url = self._image_to_base64_url(item["image"])
                        openai_content_parts.append(
                            {"type": "image_url", "image_url": {"url": base64_url}}
                        )

                    # ## MODIFIED: æ ¹æ®ç­–ç•¥å¤„ç†è§†é¢‘ ##
                    elif item_type == "video":
                        # --- ç­–ç•¥1: å°†è§†é¢‘å¸§ä½œä¸ºå›¾åƒåºåˆ— ---
                        if self.video_conversion_strategy == "as_images":
                            frames = item.get("video")
                            if frames and isinstance(frames, list):
                                openai_content_parts.append(
                                    {
                                        "type": "text",
                                        "text": "The following images are sequential frames from a video.",
                                    }
                                )
                                for frame in frames:
                                    if isinstance(frame, Image.Image):
                                        base64_url = self._image_to_base64_url(frame)
                                        openai_content_parts.append(
                                            {
                                                "type": "image_url",
                                                "image_url": {"url": base64_url},
                                            }
                                        )

                        # --- ç­–ç•¥2: å°†æ•´ä¸ªè§†é¢‘ä½œä¸º video_url (vLLM æ‰©å±•) ---
                        elif self.video_conversion_strategy == "as_video_url":
                            # è¿™ç§ç­–ç•¥è¦æ±‚è¾“å…¥çš„æ˜¯è§†é¢‘è·¯å¾„æˆ–URLï¼Œè€Œä¸æ˜¯å¸§
                            video_path = item.get("video_path")
                            video_url = item.get("video_url")

                            if video_path and os.path.exists(video_path):
                                with open(video_path, "rb") as video_file:
                                    video_bytes = video_file.read()
                                # æ¨æ–­mime typeï¼Œè¿™é‡Œç®€åŒ–ä¸ºmp4
                                final_url = self._data_to_base64_url(
                                    video_bytes, "video/mp4"
                                )
                                openai_content_parts.append(
                                    {
                                        "type": "video_url",
                                        "video_url": {"url": final_url},
                                    }
                                )
                            elif video_url:
                                # å¦‚æœç›´æ¥æä¾›äº†URLï¼Œåˆ™ç›´æ¥ä½¿ç”¨
                                openai_content_parts.append(
                                    {
                                        "type": "video_url",
                                        "video_url": {"url": video_url},
                                    }
                                )

                if openai_content_parts:
                    openai_messages.append(
                        {"role": role, "content": openai_content_parts}
                    )

        return openai_messages

    # --- NEW: æ—¥å¿—å‡€åŒ–å·¥å…·æ–¹æ³• ---
    def _sanitize_payload_for_logging(
        self, payload: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºä¸€ä¸ªpayloadçš„æ·±æ‹·è´ï¼Œå¹¶ç”¨å ä½ç¬¦æ›¿æ¢å…¶ä¸­å¤§çš„å¤šåª’ä½“æ•°æ®ï¼Œä»¥ä¾¿æ¸…æ™°åœ°æ‰“å°æ—¥å¿—ã€‚
        """
        # ä½¿ç”¨æ·±æ‹·è´ä»¥ç¡®ä¿ä¸ä¿®æ”¹åŸå§‹payload
        sanitized_payload = copy.deepcopy(payload)

        for message in sanitized_payload:
            if isinstance(message.get("content"), list):
                # ç»Ÿè®¡å›¾åƒå¸§æ•°é‡
                new_content = []
                muilt_frames_begin = False
                nframes = 0
                for part in message["content"]:
                    # å¦‚æœæ˜¯å›¾åƒURLï¼Œæ›¿æ¢å…¶å†…å®¹
                    if part.get("type") == "image_url" and "url" in part.get(
                        "image_url", {}
                    ):
                        muilt_frames_begin = True
                        nframes += 1
                    # ä¿æŒæ–‡æœ¬å†…å®¹ä¸å˜ï¼Œä½†è·³è¿‡è§†é¢‘å¸§çš„æè¿°æ–‡æœ¬
                    else:
                        if part.get("type") == "text":
                            new_content.append(part)
                        else:
                            new_content.append(part)  # ä¿ç•™å…¶ä»–æ‰€æœ‰éƒ¨åˆ†

                        if muilt_frames_begin:
                            muilt_frames_begin = False
                            new_content.append(
                                {
                                    "type": "text",
                                    "text": f"<image: {nframes} frames>",
                                }
                            )
                            nframes = 0
                if nframes > 0:
                    new_content.append(
                        {
                            "type": "text",
                            "text": f"<image: {nframes} frames>",
                        }
                    )
                message["content"] = new_content
        return sanitized_payload

    @retry(
        max_attempts=3,
        delay_seconds=1.0,
        exceptions_to_retry=(APIError, RateLimitError, AuthenticationError),
    )
    @with_metrics(metrics=["time"])
    def generate(
        self, history: List[Dict[str, Any]], max_tokens: int = 2048, **kwargs
    ) -> Tuple[str, Any]:
        messages = self._convert_history_to_openai_input(history)
        sanitized_messages_for_log = self._sanitize_payload_for_logging(messages)
        print("\n--- [OpenAIAdapter] Sending Payload ---")
        try:
            json_str = json.dumps(
                sanitized_messages_for_log, indent=2, ensure_ascii=False
            )
            print(json_str.replace("\\n", "\n"))
        except TypeError:
            print(sanitized_messages_for_log)
        print("-------------------------------------\n")
        completion = self.client.chat.completions.create(
            model=self.model_name, messages=messages, max_tokens=max_tokens, **kwargs
        )
        response = completion.choices[0].message.content or ""
        print(f"\n--- [OpenAIAdapter] Received Response ---\n{response.strip()}\n")
        return response.strip(), completion


def resize_image_by_short_side(image: Image.Image, target_size: int) -> Image.Image:
    """
    æŒ‰ç…§çŸ­è¾¹è¿›è¡Œå›¾åƒç¼©æ”¾

    Args:
        image: PILå›¾åƒå¯¹è±¡
        target_size: ç›®æ ‡çŸ­è¾¹å°ºå¯¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®ä¾‹çš„target_size

    Returns:
        ç¼©æ”¾åçš„PILå›¾åƒå¯¹è±¡
    """
    width, height = image.size

    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆæŒ‰çŸ­è¾¹ï¼‰
    if width < height:
        # å®½åº¦æ˜¯çŸ­è¾¹
        scale = target_size / width
        new_width = target_size
        new_height = int(height * scale)
    else:
        # é«˜åº¦æ˜¯çŸ­è¾¹
        scale = target_size / height
        new_height = target_size
        new_width = int(width * scale)

    # ä½¿ç”¨é«˜è´¨é‡çš„é‡é‡‡æ ·æ–¹æ³•
    resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    return resized_image


def extract_frames_with_decord(
    video_path: str, frames_per_second: int = 1
) -> List[Image.Image]:
    import decord

    """Extracts frames using Decord, which is generally faster."""
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"Video file not found: {video_path}")

    # decord.bridge.set_bridge('torch') # Optional: for PyTorch tensors
    vr = decord.VideoReader(video_path)
    video_fps = vr.get_avg_fps()
    frame_interval = int(video_fps / frames_per_second) if video_fps > 0 else 1
    if frame_interval == 0:
        frame_interval = 1

    # More efficient way to get frames with decord
    frame_indices = list(range(0, len(vr), frame_interval))
    frames_data = vr.get_batch(
        frame_indices
    ).asnumpy()  # Get all frames at once as a NumPy array

    return [
        resize_image_by_short_side(Image.fromarray(frame_np), 256)
        for frame_np in frames_data
    ]


if __name__ == "__main__":
    from robot_brain_system.core.brain import BrainMemory

    """
    ç‹¬ç«‹çš„æµ‹è¯•å…¥å£ï¼Œç”¨äºéªŒè¯å„ä¸ªé€‚é…å™¨æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œã€‚
    """
    parser = argparse.ArgumentParser(description="ç‹¬ç«‹çš„æ¨¡å‹é€‚é…å™¨æµ‹è¯•è„šæœ¬ã€‚")
    parser.add_argument(
        "--adapter",
        type=str,
        required=True,
        choices=["transformers", "vllm", "lmdeploy", "openai"],
        help="è¦æµ‹è¯•çš„é€‚é…å™¨ç±»å‹ã€‚",
    )
    parser.add_argument(
        "--model_path",
        type=str,
        help="æœ¬åœ°æ¨¡å‹çš„è·¯å¾„ (transformers, vllm, lmdeployéœ€è¦)ã€‚",
    )
    parser.add_argument(
        "--model_name",
        type=str,
        default="",
        help="æ¨¡å‹åç§° (OpenAIéœ€è¦)ï¼Œä¾‹å¦‚ 'gpt-4-vision-preview'ã€‚",
    )
    parser.add_argument(
        "--api_key", type=str, default="123456", help="APIå¯†é’¥ (OpenAIéœ€è¦)ã€‚"
    )
    parser.add_argument(
        "--base_url", type=str, default="", help="APIçš„åŸºç¡€URL (OpenAIå¯é€‰)ã€‚"
    )

    args = parser.parse_args()

    # --- 1. åˆ›å»ºæµ‹è¯•æ•°æ® ---
    print("æ­£åœ¨åˆ›å»ºç”¨äºæµ‹è¯•çš„è™šæ‹Ÿçº¢è‰²å›¾ç‰‡...")
    dummy_image = Image.open(
        "./logs/20250710_213819/1_0_monitor_press_button_input_5.png"
    )
    print("è™šæ‹Ÿå›¾ç‰‡åˆ›å»ºå®Œæ¯•ã€‚")

    video_frames = extract_frames_with_decord(
        "./thirdparty/sam2/notebooks/videos/bedroom.mp4", frames_per_second=1
    )
    # æµ‹è¯•ç”¨ä¾‹1: å¤æ‚çš„å¤šè½®å¯¹è¯ï¼ŒåŒ…å«å›¾ç‰‡
    multi_turn_image_memory = BrainMemory()
    multi_turn_image_memory.add_system_prompt(
        "You are a helpful assistant for object detection."
    )
    multi_turn_image_memory.add_user_input(
        contents=[
            "Detect output bbox of the red box in the image. The format of output should be like {'bbox_2d': [x1, y1, x2, y2], 'label': 'red box'}.",
            dummy_image,
        ]
    )
    multi_turn_image_memory.add_user_input(
        contents=["that is one the red box? what is silver object?", dummy_image]
    )

    # æµ‹è¯•ç”¨ä¾‹2: è§†é¢‘ä½œä¸ºå›¾åƒå¸§åºåˆ—
    video_as_images_memory = BrainMemory()
    video_as_images_memory.add_user_input(
        contents=[
            "Describe the content of this video based on its frames.",
            video_frames,
        ]
    )

    test_cases = []
    test_cases.append(
        {"name": "Multi-turn Image Test", "memory": multi_turn_image_memory}
    )
    test_cases.append(
        {"name": "Video as Image Frames Test", "memory": video_as_images_memory}
    )

    # --- 3. åˆå§‹åŒ–é€‰æ‹©çš„é€‚é…å™¨ ---
    adapter: Optional[BaseModelAdapter] = None
    print(f"\næ­£åœ¨åˆå§‹åŒ–é€‚é…å™¨: {args.adapter}...")
    try:
        if args.adapter == "transformers":
            adapter = TransformersAdapter(model_path=args.model_path)
        elif args.adapter == "vllm":
            adapter = VLLMAdapter(model_path=args.model_path)
        elif args.adapter == "lmdeploy":
            adapter = LMDeployAdapter(model_path=args.model_path)
        elif args.adapter == "openai":
            # ## MODIFIED: ä½¿ç”¨æ–°çš„ openai_video_strategy å‚æ•°
            adapter = OpenAIAdapter(
                model_name=args.model_name,
                api_key=args.api_key,
                base_url=args.base_url,
            )
        print("é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"\n--- âŒ åˆå§‹åŒ–é”™è¯¯ ---\nåˆå§‹åŒ–é€‚é…å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        exit(1)

    # --- 4. æ‰§è¡Œç”Ÿæˆå¹¶æ‰“å°ç»“æœ ---
    if adapter:
        print("\n--- ğŸš€ å¼€å§‹ç”Ÿæˆå“åº” ---")
        try:
            for test_case in test_cases:
                name = test_case["name"]
                memory = test_case["memory"]
                print(f"\n{'=' * 20} Running Test Case: {name} {'=' * 20}")

                response_text, raw_output = adapter.generate(
                    history=memory.history, max_tokens=256
                )

                print("\n--- âœ… æµ‹è¯•æˆåŠŸ ---")
                print(f"\nğŸ¤– æ¨¡å‹å¯¹ '{name}' çš„å“åº”:")
                print("-" * 20)
                print(response_text)
                print("-" * 20)

        except Exception as e:
            import traceback

            print(f"\n--- âŒ ç”Ÿæˆé”™è¯¯ ---\nç”Ÿæˆå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            traceback.print_exc()
            exit(1)
