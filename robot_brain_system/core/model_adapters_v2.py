# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€çš„æ¨¡å‹é€‚é…å™¨æ¨¡å—ï¼ˆå·²ä¿®æ­£ç‰ˆæœ¬ï¼‰ï¼Œä¸ºä¸åŒæ¨ç†åç«¯æä¾›æ ‡å‡†åŒ–æ¥å£ã€‚
"""

import base64
from abc import ABC, abstractmethod
from io import BytesIO
from typing import Any, Dict, List, Optional, Tuple
import argparse

import torch
from PIL import Image

# --- ä¾èµ–å¯¼å…¥ï¼Œå¸¦é”™è¯¯å¤„ç† ---
try:
    from transformers import (AutoConfig, AutoModelForCausalLM, AutoProcessor,
                              Glm4vForConditionalGeneration)
    # æ˜¾å¼å¯¼å…¥ Qwen2.5-VL çš„æ¨¡å‹ç±»ï¼Œç¡®ä¿é€»è¾‘å®Œæ•´
    from transformers.models.qwen2_5_vl import \
        Qwen2_5_VLForConditionalGeneration
    from qwen_vl_utils import process_vision_info
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    class AutoProcessor: pass
    class Glm4vForConditionalGeneration: pass
    class Qwen2_5_VLForConditionalGeneration: pass
    def process_vision_info(*args, **kwargs): return [], []

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from lmdeploy import GenerationConfig, pipeline
    from lmdeploy.vl.constants import IMAGE_TOKEN
    LMDEPLOY_AVAILABLE = True
except ImportError:
    LMDEPLOY_AVAILABLE = False
    IMAGE_TOKEN = "<image>"

try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False


# --- æŠ½è±¡åŸºç±» ---
class BaseModelAdapter(ABC):
    @abstractmethod
    def generate(
        self,
        history: List[Dict[str, Any]],
        max_tokens: int = 2048,
        **kwargs,
    ) -> Tuple[str, Any]:
        pass


# --- å››ç§é€‚é…å™¨å®ç°ï¼ˆä¿®æ­£ç‰ˆï¼‰ ---

class TransformersAdapter(BaseModelAdapter):
    """
    åŸºäº Hugging Face `transformers` åº“çš„é€‚é…å™¨ï¼ˆå·²ä¿®æ­£æ¨¡å‹åŠ è½½å’Œè§£ç é€»è¾‘ï¼‰ã€‚
    """
    def __init__(self, model_path: str, device: str = "auto"):
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("`transformers` å’Œ `qwen_vl_utils` æ˜¯ä½¿ç”¨ TransformersAdapter çš„å¿…è¦ä¾èµ–ã€‚")

        self.model_path = model_path
        self.device = device
        self.processor = AutoProcessor.from_pretrained(model_path, use_fast=True)
        
        # FIX: ä¿®æ­£äº†æ¨¡å‹åŠ è½½é€»è¾‘ï¼Œä½¿ç”¨ config åˆ¤æ–­å¹¶åŠ è½½æ­£ç¡®çš„æ¨¡å‹ç±»
        config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        model_type = getattr(config, "model_type", "").lower()
        print(model_type)

        if "qwen2_5_vl" in model_type:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True
            )
            self._handler = self._generate_qwen
            print("[TransformersAdapter] å·²åŠ è½½ Qwen2.5-VL æ¨¡å‹ã€‚")
        elif "glm4v" in model_type:
            self.model = Glm4vForConditionalGeneration.from_pretrained(
                model_path, torch_dtype=torch.bfloat16, device_map=device
            )
            self._handler = self._generate_generic
            print("[TransformersAdapter] å·²åŠ è½½ GLM-4V æ¨¡å‹ã€‚")
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True
            )
            self._handler = self._generate_generic
            print(f"[TransformersAdapter] å·²åŠ è½½é€šç”¨æ¨¡å‹: {model_type}ã€‚")

    def _generate_qwen(self, history: List[Dict[str, Any]], gen_kwargs: Dict) -> Tuple[torch.Tensor, Dict]:
        text_prompt = self.processor.apply_chat_template(
            history, tokenize=False, add_generation_prompt=True
        )
        vision_outputs = process_vision_info(history)
        images, videos = (vision_outputs[0], vision_outputs[1]) if len(vision_outputs) >= 2 else ([], [])

        inputs = self.processor(
            text=[text_prompt], images=images, videos=videos, padding=True, return_tensors="pt"
        ).to(self.model.device)
        
        generated_ids = self.model.generate(**inputs, **gen_kwargs)
        return generated_ids, inputs

    def _generate_generic(self, history: List[Dict[str, Any]], gen_kwargs: Dict) -> Tuple[torch.Tensor, Dict]:
        inputs = self.processor.apply_chat_template(
            history,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
            padding=True,
            do_sample_frames=True
        ).to(self.model.device)
        generated_ids = self.model.generate(**inputs, **gen_kwargs)
        return generated_ids, inputs

    def generate(
        self, history: List[Dict[str, Any]], max_tokens: int = 2048, **kwargs
    ) -> Tuple[str, Any]:
        
        print(f'\n\n{"="*20}input{"="*20}')
        print(f"{history}")
        
        gen_kwargs = {"max_new_tokens": max_tokens,"top_k":2, "repetition_penalty": 1.0, "temperature": 1.0, "do_sample": True}
        gen_kwargs.update(kwargs)
        
        # FIX: _handler ç°åœ¨è¿”å› (generated_ids, inputs) ä»¥ç¡®ä¿ input_token_len å‡†ç¡®
        generated_ids, inputs = self._handler(history, gen_kwargs)
        
        input_token_len = inputs.input_ids.shape[1]
        response_ids = generated_ids[0, input_token_len:]
        response = self.processor.decode(response_ids, skip_special_tokens=True)
        
        print(f'\n\n{"="*20}response{"="*20}')
        print(f"{response.strip()}")
        
        return response.strip(), generated_ids

class VLLMAdapter(BaseModelAdapter):
    """åŸºäº `vLLM` çš„é«˜æ•ˆæ¨ç†é€‚é…å™¨ã€‚"""
    def __init__(self, model_path: str, **vllm_kwargs):
        if not VLLM_AVAILABLE:
            raise ImportError("`vllm` æ˜¯ä½¿ç”¨ VLLMAdapter çš„å¿…è¦ä¾èµ–ã€‚")
        vllm_kwargs.setdefault("trust_remote_code", True)
        self.llm = LLM(model=model_path, **vllm_kwargs)
        self.tokenizer = self.llm.get_tokenizer()

    def _convert_history_to_vllm_input(self, history: List[Dict[str, Any]]) -> Dict[str, Any]:
        images, processed_history = [], []
        for message in history:
            role, content = message["role"], message.get("content", [])
            new_content_parts = []
            for item in content:
                if item["type"] == "text":
                    new_content_parts.append(item["text"])
                elif item["type"] in ["image", "video"]:
                    frames = item.get("video", [item.get("image")])
                    for frame in frames:
                        if frame:
                            new_content_parts.append("<image>")
                            images.append(frame)
            processed_history.append({"role": role, "content": "".join(new_content_parts)})

        prompt = self.tokenizer.apply_chat_template(
            conversation=processed_history, tokenize=False, add_generation_prompt=True
        )
        return {"prompt": prompt, "multi_modal_data": {"image": images} if images else None}

    def generate(self, history: List[Dict[str, Any]], max_tokens: int = 2048, **kwargs) -> Tuple[str, Any]:
        vllm_input = self._convert_history_to_vllm_input(history)
        sampling_params = SamplingParams(max_tokens=max_tokens, **kwargs)
        outputs = self.llm.generate(
            prompts=[vllm_input["prompt"]],
            sampling_params=sampling_params,
            multi_modal_data=vllm_input.get("multi_modal_data"),
        )
        response = outputs[0].outputs[0].text
        return response.strip(), outputs

class LMDeployAdapter(BaseModelAdapter):
    """åŸºäº `lmdeploy` pipeline çš„æ¨ç†é€‚é…å™¨ï¼ˆå·²ä¿®æ­£æ•°æ®è½¬æ¢é€»è¾‘ï¼‰ã€‚"""
    def __init__(self, model_path: str, **pipeline_kwargs):
        if not LMDEPLOY_AVAILABLE:
            raise ImportError("`lmdeploy` æ˜¯ä½¿ç”¨ LMDeployAdapter çš„å¿…è¦ä¾èµ–ã€‚")
        self.pipe = pipeline(model_path, **pipeline_kwargs)

    def generate(self, history: List[Dict[str, Any]], max_tokens: int = 2048, **kwargs) -> Tuple[str, Any]:
        # FIX: lmdeploy çš„ pipeline å¯ä»¥ç›´æ¥å¤„ç†æ ‡å‡† history æ ¼å¼ï¼Œæ— éœ€å¤æ‚è½¬æ¢
        # å®ƒä¼šè‡ªåŠ¨å¤„ç† content åˆ—è¡¨ä¸­çš„æ–‡æœ¬å’Œå›¾åƒ
        gen_config = GenerationConfig(max_new_tokens=max_tokens, **kwargs)
        output = self.pipe(history, gen_config=gen_config)
        response = output.text if hasattr(output, 'text') else str(output)
        return response.strip(), output

class OpenAIAdapter(BaseModelAdapter):
    """é€‚é…å™¨ï¼Œç”¨äºè°ƒç”¨ç¬¦åˆ OpenAI API è§„èŒƒçš„æ¨¡å‹æœåŠ¡ï¼ˆå·²ä¼˜åŒ–å›¾åƒå¤„ç†ï¼‰ã€‚"""
    def __init__(self, model_name: str, api_key: str, base_url: Optional[str] = None):
        if not OPENAI_AVAILABLE:
            raise ImportError("`openai` æ˜¯ä½¿ç”¨ OpenAIAdapter çš„å¿…è¦ä¾èµ–ã€‚")
        self.client = openai.OpenAI(api_key=api_key, base_url=base_url)
        self.model_name = model_name

    def _image_to_base64_url(self, image: Image.Image) -> str:
        buffered = BytesIO()
        # FIX: è½¬æ¢ä¸ºRGBä»¥å¤„ç†é€æ˜é€šé“ï¼Œå¢åŠ ä»£ç å¥å£®æ€§
        image.convert("RGB").save(buffered, format="JPEG")
        img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
        return f"data:image/jpeg;base64,{img_str}"

    def _convert_history_to_openai_input(self, history: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        openai_messages = []
        for message in history:
            role, content = message["role"], message.get("content", [])
            if role in ["system", "assistant"]:
                text_content = "".join([item["text"] for item in content if item["type"] == "text"])
                openai_messages.append({"role": role, "content": text_content})
            elif role == "user":
                openai_content = []
                for item in content:
                    if item["type"] == "text":
                        openai_content.append({"type": "text", "text": item["text"]})
                    elif item["type"] in ["image", "video"]:
                        frames = item.get("video", [item.get("image")])
                        for frame in frames:
                            if frame:
                                base64_url = self._image_to_base64_url(frame)
                                openai_content.append({"type": "image_url", "image_url": {"url": base64_url}})
                openai_messages.append({"role": role, "content": openai_content})
        return openai_messages

    def generate(self, history: List[Dict[str, Any]], max_tokens: int = 2048, **kwargs) -> Tuple[str, Any]:
        messages = self._convert_history_to_openai_input(history)
        completion = self.client.chat.completions.create(
            model=self.model_name, messages=messages, max_tokens=max_tokens, **kwargs
        )
        response = completion.choices[0].message.content
        return response.strip(), completion
    

if __name__ == '__main__':
    """
    ç‹¬ç«‹çš„æµ‹è¯•å…¥å£ï¼Œç”¨äºéªŒè¯å„ä¸ªé€‚é…å™¨æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œã€‚
    """
    parser = argparse.ArgumentParser(description="ç‹¬ç«‹çš„æ¨¡å‹é€‚é…å™¨æµ‹è¯•è„šæœ¬ã€‚")
    parser.add_argument(
        "--adapter",
        type=str,
        required=True,
        choices=["transformers", "vllm", "lmdeploy", "openai"],
        help="è¦æµ‹è¯•çš„é€‚é…å™¨ç±»å‹ã€‚"
    )
    parser.add_argument("--model_path", type=str, help="æœ¬åœ°æ¨¡å‹çš„è·¯å¾„ (transformers, vllm, lmdeployéœ€è¦)ã€‚")
    parser.add_argument("--model_name", type=str, default="", help="æ¨¡å‹åç§° (OpenAIéœ€è¦)ï¼Œä¾‹å¦‚ 'gpt-4-vision-preview'ã€‚")
    parser.add_argument("--api_key", type=str, default="", help="APIå¯†é’¥ (OpenAIéœ€è¦)ã€‚")
    parser.add_argument("--base_url", type=str,default="", help="APIçš„åŸºç¡€URL (OpenAIå¯é€‰)ã€‚")

    args = parser.parse_args()

    # --- 1. åˆ›å»ºæµ‹è¯•æ•°æ® ---
    print("æ­£åœ¨åˆ›å»ºç”¨äºæµ‹è¯•çš„è™šæ‹Ÿçº¢è‰²å›¾ç‰‡...")
    dummy_image = Image.open('/data/shiqi/IsaacLab/logs/20250710_110850/1_0_monitor_press_button_input_5.png')
    print("è™šæ‹Ÿå›¾ç‰‡åˆ›å»ºå®Œæ¯•ã€‚")

    test_history = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Detect output bbox of red box and yellow buttonï¼Œ The format of output should be like {â€œbbox_2dâ€: [x1, y1, x2, y2], â€œlabelâ€: â€œmotorcyclistâ€, â€œsub_labelâ€: â€œwearing helmatâ€ # or â€œnot wearing helmatâ€}."},
                {"type": "image", "image": dummy_image}
            ]
        }
    ]
    print("æµ‹è¯•å¯¹è¯å†å²å·²å‡†å¤‡å¥½ã€‚")

    # --- 2. åˆå§‹åŒ–é€‰æ‹©çš„é€‚é…å™¨ ---
    adapter: Optional[BaseModelAdapter] = None
    print(f"\næ­£åœ¨åˆå§‹åŒ–é€‚é…å™¨: {args.adapter}...")
    try:
        if args.adapter == "transformers":
            if not args.model_path: raise ValueError("--model_path æ˜¯ transformers é€‚é…å™¨æ‰€å¿…éœ€çš„ã€‚")
            adapter = TransformersAdapter(model_path=args.model_path)
        elif args.adapter == "vllm":
            if not args.model_path: raise ValueError("--model_path æ˜¯ vllm é€‚é…å™¨æ‰€å¿…éœ€çš„ã€‚")
            adapter = VLLMAdapter(model_path=args.model_path)
        elif args.adapter == "lmdeploy":
            if not args.model_path: raise ValueError("--model_path æ˜¯ lmdeploy é€‚é…å™¨æ‰€å¿…éœ€çš„ã€‚")
            adapter = LMDeployAdapter(model_path=args.model_path)
        elif args.adapter == "openai":
            if not args.model_name or not args.api_key:
                raise ValueError("--model_name å’Œ --api_key æ˜¯ openai é€‚é…å™¨æ‰€å¿…éœ€çš„ã€‚")
            adapter = OpenAIAdapter(model_name=args.model_name, api_key=args.api_key, base_url=args.base_url)
        
        print("é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸï¼")

    except Exception as e:
        print(f"\n--- âŒ åˆå§‹åŒ–é”™è¯¯ ---")
        print(f"åˆå§‹åŒ–é€‚é…å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        exit(1)

    # --- 3. æ‰§è¡Œç”Ÿæˆå¹¶æ‰“å°ç»“æœ ---
    if adapter:
        print("\n--- ğŸš€ å¼€å§‹ç”Ÿæˆå“åº” ---")
        try:
            response_text, raw_output = adapter.generate(history=test_history, max_tokens=2048)
            
            print("\n--- âœ… æµ‹è¯•æˆåŠŸ ---")
            print("\nğŸ¤– æ¨¡å‹å“åº”:")
            print("="*20)
            print(response_text)
            print("="*20)

        except Exception as e:
            import traceback
            print(f"\n--- âŒ ç”Ÿæˆé”™è¯¯ ---")
            print(f"ç”Ÿæˆå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            traceback.print_exc()
            exit(1)